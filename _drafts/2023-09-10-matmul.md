---
layout: post
title: "Matmul"
date: 2023-12-01 14:01:45 -0700
categories: hardware
---

# Matmul

Can we make matmul fast? Let's try a matmul algorithm in python.

Wow so slow! Approximately 25x slower than `@` operator in python

```python
def matmul(A: np.ndarray, B: np.ndarray) -> np.ndarray:
  # A is size m x n, B is size n x p. C (output) is size m x p. Multiply->Add
  assert A.shape[1] == B.shape[0]
  C, n = np.zeros((A.shape[0], B.shape[1])), A.shape[1]
  for i in range(A.shape[0]):
    for j in range(B.shape[0]):
      C[i][j] = sum([A[i][k] * B[k][j] for k in range(n)])
  return C
```

## Hardware

What would a hardware accelerator for a 1024x1024 matrix look like?

A matmul op is `Memory => Multipy => Sum => Memory`. A binary and reduce op, with a storage read and write.

Let's start with storage. Today's GPUs use GDDR6 SGRAM (Synchronous Graphics Random Access Memory). However, cutting edge ML/AI accelerators, like the H100 use HBM2E memory, a high bandwidth memory option.

A 1024 x 1024 float32 matrix takes 4.2 megabytes in RAM. Micron's off the shelf [GDDR6 memory](https://www.arrow.com/en/products/mt61m256m32je-12-aata/micron-technology)
modules can handle throughput of 12 Gb/s, which would give us a ceiling of 357 1024^2 matrix
read/writes per second.

For each matmul, we need to load 2 matrices from RAM, compute the product, and store the result.

Hopefully we can do this with [laziness](https://en.wikipedia.org/wiki/Lazy_evaluation) so we're not making multiple
round trips to memory for each op. Let's evaluate some memory chips:

| Manufacturer | Product                                                                                  | Type        | Bandwidth |
| ------------ | ---------------------------------------------------------------------------------------- | ----------- | --------- |
| Samsung      | [K4Z80325BC-HC14](https://www.arrow.com/en/products/k4z80325bc-hc14/samsung-electronics) | GDDR6 SDRAM | 8Gb/s     |
